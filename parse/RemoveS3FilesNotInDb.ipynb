{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asos/Asos.0604.20191201000029.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asos/Asos.0604.20191201000130.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asos/Asos.0604.20191201000230.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asos/Asos.0604.20191201000329.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asos/Asos.0604.20191201000430.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        FileLocation\n",
       "0  asos/Asos.0604.20191201000029.csv\n",
       "1  asos/Asos.0604.20191201000130.csv\n",
       "2  asos/Asos.0604.20191201000230.csv\n",
       "3  asos/Asos.0604.20191201000329.csv\n",
       "4  asos/Asos.0604.20191201000430.csv"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import mysql.connector\n",
    "# this will try to make a connection to a database in AWS\n",
    "try:\n",
    "    mydb = mysql.connector.connect(\n",
    "      host=\"localhost\", # URL or hostname of the database in Oregon  \n",
    "      user=\"admin\",\n",
    "      password=\"34FEJe?jq34!!0f\",\n",
    "      database='rwdp')\n",
    "# if an error occurs it will print out the problem    \n",
    "except mysql.connector.Error as err:\n",
    "    print(\"Something went wrong: {}\".format(err))\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "### Customize this line per file type\n",
    "query = \"SELECT FileLocation from rwdp.AsosMeasurements\"\n",
    "\n",
    "\n",
    "mycursor.execute(query)\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "headers = ['FileLocation']\n",
    "\n",
    "dbfiles_df = pd.DataFrame(data = myresult, \n",
    "                 columns = headers)\n",
    "# 6. Close the database\n",
    "# It's# 6. Close the database good practice to close the database connection once you are done pulling data    \n",
    "mydb.close()\n",
    "\n",
    "dbfiles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asos/Asos.0603.20191231000029.csv\n",
      "asos/Asos.0603.20191231000129.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "session = boto3.Session(\n",
    "    region_name='us-gov-west-1',\n",
    "    profile_name='vafb'\n",
    ")\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket('dxhub-vafb-xui-weather-data-raw')\n",
    "## List objects within a given prefix\n",
    "s3files = []\n",
    "for obj in bucket.objects.filter(Delimiter='/', Prefix='asos/'):\n",
    "    s3files.append(obj.key)\n",
    "\n",
    "s3files_df = pd.DataFrame(data = s3files, \n",
    "                 columns = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign location to each dataframe that will we use in the merge\n",
    "s3files_df['location'] = 's3'\n",
    "dbfiles_df['location'] = 'db'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1138089, 2)\n",
      "(914617, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileLocation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asos/Asos.0603.20191231000029.csv</td>\n",
       "      <td>s3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asos/Asos.0603.20191231000129.csv</td>\n",
       "      <td>s3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asos/Asos.0603.20191231000229.csv</td>\n",
       "      <td>s3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asos/Asos.0603.20191231000329.csv</td>\n",
       "      <td>s3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asos/Asos.0603.20191231000429.csv</td>\n",
       "      <td>s3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        FileLocation location\n",
       "0  asos/Asos.0603.20191231000029.csv       s3\n",
       "1  asos/Asos.0603.20191231000129.csv       s3\n",
       "2  asos/Asos.0603.20191231000229.csv       s3\n",
       "3  asos/Asos.0603.20191231000329.csv       s3\n",
       "4  asos/Asos.0603.20191231000429.csv       s3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (s3files_df.shape)\n",
    "print (dbfiles_df.shape)\n",
    "s3files_df.head()\n",
    "#dbfiles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files_that_exist_on_s3_not_in_db (223472, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileLocation</th>\n",
       "      <th>location_x</th>\n",
       "      <th>location_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>asos/Asos.0603.20200201010529.csv</td>\n",
       "      <td>s3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>asos/Asos.0603.20200201011329.csv</td>\n",
       "      <td>s3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>asos/Asos.0603.20200201011529.csv</td>\n",
       "      <td>s3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>asos/Asos.0603.20200201012029.csv</td>\n",
       "      <td>s3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>asos/Asos.0603.20200201012829.csv</td>\n",
       "      <td>s3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           FileLocation location_x location_y\n",
       "1505  asos/Asos.0603.20200201010529.csv         s3        NaN\n",
       "1513  asos/Asos.0603.20200201011329.csv         s3        NaN\n",
       "1515  asos/Asos.0603.20200201011529.csv         s3        NaN\n",
       "1520  asos/Asos.0603.20200201012029.csv         s3        NaN\n",
       "1528  asos/Asos.0603.20200201012829.csv         s3        NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want a list of files that are on S3 but NOT in the DB, those files need to be deleted\n",
    "# Let's do a left merge so that any files cleans will show up as NaN\n",
    "files_merged = s3files_df.merge(dbfiles_df, on='FileLocation', how='left')\n",
    "\n",
    "\n",
    "files_not_in_db_filter = files_merged['location_y'].isnull()\n",
    "files_that_exist_on_s3_not_in_db = files_merged[files_not_in_db_filter]\n",
    "print ('files_that_exist_on_s3_not_in_db', files_that_exist_on_s3_not_in_db.shape)\n",
    "files_that_exist_on_s3_not_in_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asos/Asos.0603.20200201011529.csv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_that_exist_on_s3_not_in_db.iloc[2]['FileLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SDK to remove all files from S3 not in the DB\n",
    "session = boto3.Session(\n",
    "    region_name='us-gov-west-1',\n",
    "    profile_name='vafb'\n",
    ")\n",
    "\n",
    "client = session.client('s3')\n",
    "for objects in files_that_exist_on_s3_not_in_db.iterrows():\n",
    "    key = objects[1]['FileLocation']\n",
    "    response = client.delete_object(Bucket='dxhub-vafb-xui-weather-data-raw',Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
